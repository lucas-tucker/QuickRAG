{"docstore/metadata": {"5018da1e-a4f1-4731-b55c-151776e5927a": {"doc_hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108"}, "424f04aa-2b6e-4f9b-8141-549b4bd43b9d": {"doc_hash": "0dc5b0c9ddf5e8bdcb105291ce9fd6fe77a86eac3a835b235fd9f2e5c00acf53", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "4562a98a-6684-4c1d-9d47-ad75db7b64da": {"doc_hash": "05c6c847f4106f44ac660e4ba12330c2b1b840a186f8f569c1995befbe3c3360", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "f0d36b74-c870-4c87-9be4-dd3a94f30107": {"doc_hash": "4827fe44bf15a08665c0e050db14ddc36a4fc734adb19e96d5ea52293573397c", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "5346372c-a1e8-4904-89d3-ff2c9bf63223": {"doc_hash": "1777fd825fbf5d4a9c5971c9440175f460146702a45f8a28a8bf5e1202d439bc", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "6a391770-56cc-4ef2-b9b5-e9bac03c2ed0": {"doc_hash": "18704562177b3c4f7a056449d2d6630cfac87aee017f18ede554d09488fb03d7", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "64abcaea-e874-407d-a515-186b2a75e80b": {"doc_hash": "834c303560453979ce2dc4e7c9433c5c6d3f4c3564d2b8ad1e5148d9f2d56a1a", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "4dc6ee0c-973a-4a88-a61d-be060d359dd3": {"doc_hash": "4b0310c7414e1d0d010bc1d019b4046fcf4d0330f2d6edd9862b3c6e084644de", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "5cba0685-42ff-4095-9e19-5b303ea80d5a": {"doc_hash": "7e891ee14d5f14db7df7f449ddf8601e536b2507abb0df8381f93aa4c9cbeab2", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "67cd0149-92f8-4aed-b957-7467813c2524": {"doc_hash": "1b386e8c01ebbc2a0d8acd7ee48e7311be9688a0c74709cae8234325ae6250b6", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "1f11f84d-a003-4bad-82eb-597dde8622b9": {"doc_hash": "c600ff444e2e4102bea52fd126b3202b65066860d452867ce1cc852b7df2c343", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "f2d122ce-daf0-4246-b2fe-3ee781826130": {"doc_hash": "c5d494989d8163676e72b908a8dfc06f3af83629866efc6fbd47a5059e210ec7", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "6cbb2bdd-2bb4-4bc0-a125-652a4fa52114": {"doc_hash": "943cfcf6482529e2f936d5328ccc5aacbaee8109cf123b8e7e35983ad1af3cd3", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}, "e92abd3a-60ff-4cc8-a5d5-1705741432dd": {"doc_hash": "1fb69f2a59feae6d5b11d5b8f6885b6f0fd00dee78e24579d1abccfe692ca4c9", "ref_doc_id": "5018da1e-a4f1-4731-b55c-151776e5927a"}}, "docstore/data": {"424f04aa-2b6e-4f9b-8141-549b4bd43b9d": {"__data__": {"id_": "424f04aa-2b6e-4f9b-8141-549b4bd43b9d", "embedding": null, "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n", "original_text": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4562a98a-6684-4c1d-9d47-ad75db7b64da", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n", "original_text": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python. "}, "hash": "05c6c847f4106f44ac660e4ba12330c2b1b840a186f8f569c1995befbe3c3360", "class_name": "RelatedNodeInfo"}}, "hash": "0dc5b0c9ddf5e8bdcb105291ce9fd6fe77a86eac3a835b235fd9f2e5c00acf53", "text": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n", "start_char_idx": 0, "end_char_idx": 269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4562a98a-6684-4c1d-9d47-ad75db7b64da": {"__data__": {"id_": "4562a98a-6684-4c1d-9d47-ad75db7b64da", "embedding": null, "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n", "original_text": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "424f04aa-2b6e-4f9b-8141-549b4bd43b9d", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n", "original_text": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n"}, "hash": "0dc5b0c9ddf5e8bdcb105291ce9fd6fe77a86eac3a835b235fd9f2e5c00acf53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d36b74-c870-4c87-9be4-dd3a94f30107", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n", "original_text": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n"}, "hash": "4827fe44bf15a08665c0e050db14ddc36a4fc734adb19e96d5ea52293573397c", "class_name": "RelatedNodeInfo"}}, "hash": "05c6c847f4106f44ac660e4ba12330c2b1b840a186f8f569c1995befbe3c3360", "text": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python. ", "start_char_idx": 269, "end_char_idx": 433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0d36b74-c870-4c87-9be4-dd3a94f30107": {"__data__": {"id_": "f0d36b74-c870-4c87-9be4-dd3a94f30107", "embedding": null, "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n", "original_text": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4562a98a-6684-4c1d-9d47-ad75db7b64da", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n", "original_text": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python. "}, "hash": "05c6c847f4106f44ac660e4ba12330c2b1b840a186f8f569c1995befbe3c3360", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5346372c-a1e8-4904-89d3-ff2c9bf63223", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n", "original_text": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n"}, "hash": "1777fd825fbf5d4a9c5971c9440175f460146702a45f8a28a8bf5e1202d439bc", "class_name": "RelatedNodeInfo"}}, "hash": "4827fe44bf15a08665c0e050db14ddc36a4fc734adb19e96d5ea52293573397c", "text": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n", "start_char_idx": 433, "end_char_idx": 501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5346372c-a1e8-4904-89d3-ff2c9bf63223": {"__data__": {"id_": "5346372c-a1e8-4904-89d3-ff2c9bf63223", "embedding": null, "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n", "original_text": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0d36b74-c870-4c87-9be4-dd3a94f30107", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n", "original_text": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n"}, "hash": "4827fe44bf15a08665c0e050db14ddc36a4fc734adb19e96d5ea52293573397c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a391770-56cc-4ef2-b9b5-e9bac03c2ed0", "node_type": "1", "metadata": {"window": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n", "original_text": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n"}, "hash": "18704562177b3c4f7a056449d2d6630cfac87aee017f18ede554d09488fb03d7", "class_name": "RelatedNodeInfo"}}, "hash": "1777fd825fbf5d4a9c5971c9440175f460146702a45f8a28a8bf5e1202d439bc", "text": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n", "start_char_idx": 501, "end_char_idx": 607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a391770-56cc-4ef2-b9b5-e9bac03c2ed0": {"__data__": {"id_": "6a391770-56cc-4ef2-b9b5-e9bac03c2ed0", "embedding": null, "metadata": {"window": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n", "original_text": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5346372c-a1e8-4904-89d3-ff2c9bf63223", "node_type": "1", "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n", "original_text": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n"}, "hash": "1777fd825fbf5d4a9c5971c9440175f460146702a45f8a28a8bf5e1202d439bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64abcaea-e874-407d-a515-186b2a75e80b", "node_type": "1", "metadata": {"window": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. ", "original_text": "Parameters do not have a batch index.\n"}, "hash": "834c303560453979ce2dc4e7c9433c5c6d3f4c3564d2b8ad1e5148d9f2d56a1a", "class_name": "RelatedNodeInfo"}}, "hash": "18704562177b3c4f7a056449d2d6630cfac87aee017f18ede554d09488fb03d7", "text": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n", "start_char_idx": 607, "end_char_idx": 701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64abcaea-e874-407d-a515-186b2a75e80b": {"__data__": {"id_": "64abcaea-e874-407d-a515-186b2a75e80b", "embedding": null, "metadata": {"window": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. ", "original_text": "Parameters do not have a batch index.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a391770-56cc-4ef2-b9b5-e9bac03c2ed0", "node_type": "1", "metadata": {"window": "For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n", "original_text": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n"}, "hash": "18704562177b3c4f7a056449d2d6630cfac87aee017f18ede554d09488fb03d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dc6ee0c-973a-4a88-a61d-be060d359dd3", "node_type": "1", "metadata": {"window": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n", "original_text": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n"}, "hash": "4b0310c7414e1d0d010bc1d019b4046fcf4d0330f2d6edd9862b3c6e084644de", "class_name": "RelatedNodeInfo"}}, "hash": "834c303560453979ce2dc4e7c9433c5c6d3f4c3564d2b8ad1e5148d9f2d56a1a", "text": "Parameters do not have a batch index.\n", "start_char_idx": 701, "end_char_idx": 739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dc6ee0c-973a-4a88-a61d-be060d359dd3": {"__data__": {"id_": "4dc6ee0c-973a-4a88-a61d-be060d359dd3", "embedding": null, "metadata": {"window": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n", "original_text": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64abcaea-e874-407d-a515-186b2a75e80b", "node_type": "1", "metadata": {"window": "On a GPU mini-\nbatching allows parallelism over the batch elements.\n 2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. ", "original_text": "Parameters do not have a batch index.\n"}, "hash": "834c303560453979ce2dc4e7c9433c5c6d3f4c3564d2b8ad1e5148d9f2d56a1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cba0685-42ff-4095-9e19-5b303ea80d5a", "node_type": "1", "metadata": {"window": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n", "original_text": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. "}, "hash": "7e891ee14d5f14db7df7f449ddf8601e536b2507abb0df8381f93aa4c9cbeab2", "class_name": "RelatedNodeInfo"}}, "hash": "4b0310c7414e1d0d010bc1d019b4046fcf4d0330f2d6edd9862b3c6e084644de", "text": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n", "start_char_idx": 739, "end_char_idx": 904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cba0685-42ff-4095-9e19-5b303ea80d5a": {"__data__": {"id_": "5cba0685-42ff-4095-9e19-5b303ea80d5a", "embedding": null, "metadata": {"window": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n", "original_text": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dc6ee0c-973a-4a88-a61d-be060d359dd3", "node_type": "1", "metadata": {"window": "2\n\nMinibatching\nWith minibatching each input value and each computed value\nis actually a batch of values.\n We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n", "original_text": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n"}, "hash": "4b0310c7414e1d0d010bc1d019b4046fcf4d0330f2d6edd9862b3c6e084644de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67cd0149-92f8-4aed-b957-7467813c2524", "node_type": "1", "metadata": {"window": "Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n", "original_text": "By convention parameter\ngradients are averaged over the batch.\n"}, "hash": "1b386e8c01ebbc2a0d8acd7ee48e7311be9688a0c74709cae8234325ae6250b6", "class_name": "RelatedNodeInfo"}}, "hash": "7e891ee14d5f14db7df7f449ddf8601e536b2507abb0df8381f93aa4c9cbeab2", "text": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. ", "start_char_idx": 904, "end_char_idx": 1353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67cd0149-92f8-4aed-b957-7467813c2524": {"__data__": {"id_": "67cd0149-92f8-4aed-b957-7467813c2524", "embedding": null, "metadata": {"window": "Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n", "original_text": "By convention parameter\ngradients are averaged over the batch.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cba0685-42ff-4095-9e19-5b303ea80d5a", "node_type": "1", "metadata": {"window": "We add a batch index as an additional \ufb01rst tensor dimension\nfor each input and computed node.\n Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n", "original_text": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements. "}, "hash": "7e891ee14d5f14db7df7f449ddf8601e536b2507abb0df8381f93aa4c9cbeab2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f11f84d-a003-4bad-82eb-597dde8622b9", "node_type": "1", "metadata": {"window": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n", "original_text": "6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n"}, "hash": "c600ff444e2e4102bea52fd126b3202b65066860d452867ce1cc852b7df2c343", "class_name": "RelatedNodeInfo"}}, "hash": "1b386e8c01ebbc2a0d8acd7ee48e7311be9688a0c74709cae8234325ae6250b6", "text": "By convention parameter\ngradients are averaged over the batch.\n", "start_char_idx": 1353, "end_char_idx": 1416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f11f84d-a003-4bad-82eb-597dde8622b9": {"__data__": {"id_": "1f11f84d-a003-4bad-82eb-597dde8622b9", "embedding": null, "metadata": {"window": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n", "original_text": "6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67cd0149-92f8-4aed-b957-7467813c2524", "node_type": "1", "metadata": {"window": "Parameters do not have a batch index.\n 3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n", "original_text": "By convention parameter\ngradients are averaged over the batch.\n"}, "hash": "1b386e8c01ebbc2a0d8acd7ee48e7311be9688a0c74709cae8234325ae6250b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2d122ce-daf0-4246-b2fe-3ee781826130", "node_type": "1", "metadata": {"window": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n"}, "hash": "c5d494989d8163676e72b908a8dfc06f3af83629866efc6fbd47a5059e210ec7", "class_name": "RelatedNodeInfo"}}, "hash": "c600ff444e2e4102bea52fd126b3202b65066860d452867ce1cc852b7df2c343", "text": "6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n", "start_char_idx": 1416, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2d122ce-daf0-4246-b2fe-3ee781826130": {"__data__": {"id_": "f2d122ce-daf0-4246-b2fe-3ee781826130", "embedding": null, "metadata": {"window": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f11f84d-a003-4bad-82eb-597dde8622b9", "node_type": "1", "metadata": {"window": "3\n\nThe Batch Index\nWe letBbe the number of elements in a single minibatch and\nletb, with 0 \u2264b\u2264B\u22121, be an index naming a particular\nelement of the current minibatch.\n 4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n", "original_text": "6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n"}, "hash": "c600ff444e2e4102bea52fd126b3202b65066860d452867ce1cc852b7df2c343", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cbb2bdd-2bb4-4bc0-a125-652a4fa52114", "node_type": "1", "metadata": {"window": "By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n"}, "hash": "943cfcf6482529e2f936d5328ccc5aacbaee8109cf123b8e7e35983ad1af3cd3", "class_name": "RelatedNodeInfo"}}, "hash": "c5d494989d8163676e72b908a8dfc06f3af83629866efc6fbd47a5059e210ec7", "text": "Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n", "start_char_idx": 1511, "end_char_idx": 1654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cbb2bdd-2bb4-4bc0-a125-652a4fa52114": {"__data__": {"id_": "6cbb2bdd-2bb4-4bc0-a125-652a4fa52114", "embedding": null, "metadata": {"window": "By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2d122ce-daf0-4246-b2fe-3ee781826130", "node_type": "1", "metadata": {"window": "4\n\nMLP with a Batch Index\nb\u2014 batch index, i\u2014 input feature index\nj\u2014 hidden layer index, \u02c6y\u2014 possible label\n\u03a6 = (W0[j,i], b0[j], W1[\u02c6y,j], b1[\u02c6y])\nh[b,j] =\u03c3\uf8eb\n\uf8ed\u2211\ni(\nW0[j,i]x[b,i])\n\u2212b0[j]\uf8f6\n\uf8f8\ns[b,\u02c6y] =\u03c3\uf8eb\n\uf8ed\u2211\nj(\nW1[\u02c6y,j]h[b,j])\n\u2212b1[\u02c6y]\uf8f6\n\uf8f8\nP\u03a6[b,\u02c6y] = softmax\n\u02c6ys[b,\u02c6y]\n\nBackpropagation with Minibatching\nforb,i,j \u02dch[b,j]+=W[j,i]x[b,i]\nforb,i,j x. grad[b,i]+=\u02dch.grad[b,j]W[j,i]\nforb,i,j W. grad[j,i]+=1\nB\u02dch.grad[b,j]x[b,i]\nBis the number of batch elements.  By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n"}, "hash": "c5d494989d8163676e72b908a8dfc06f3af83629866efc6fbd47a5059e210ec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e92abd3a-60ff-4cc8-a5d5-1705741432dd", "node_type": "1", "metadata": {"window": "6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "7\n\nEND"}, "hash": "1fb69f2a59feae6d5b11d5b8f6885b6f0fd00dee78e24579d1abccfe692ca4c9", "class_name": "RelatedNodeInfo"}}, "hash": "943cfcf6482529e2f936d5328ccc5aacbaee8109cf123b8e7e35983ad1af3cd3", "text": "But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n", "start_char_idx": 1654, "end_char_idx": 1791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e92abd3a-60ff-4cc8-a5d5-1705741432dd": {"__data__": {"id_": "e92abd3a-60ff-4cc8-a5d5-1705741432dd", "embedding": null, "metadata": {"window": "6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "7\n\nEND"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "5018da1e-a4f1-4731-b55c-151776e5927a", "node_type": "4", "metadata": {}, "hash": "314b526b4b1f3a083e1b9b07cdf754f739b82623cbb88f447c90b46ef4651108", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cbb2bdd-2bb4-4bc0-a125-652a4fa52114", "node_type": "1", "metadata": {"window": "By convention parameter\ngradients are averaged over the batch.\n 6\n\nSetting the Batch Size\nThe batch is typically made as large as the hardware will sup-\nport.\n Theoretically extremely large batches can make SGD require\nmore epochs and hence more energy consumption even for par-\nallel batch processing.\n But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n 7\n\nEND", "original_text": "But empirically, unless one has thousands of GPUs, it seems\nthe batch can be as large as possible without requiring addi-\ntional epochs.\n"}, "hash": "943cfcf6482529e2f936d5328ccc5aacbaee8109cf123b8e7e35983ad1af3cd3", "class_name": "RelatedNodeInfo"}}, "hash": "1fb69f2a59feae6d5b11d5b8f6885b6f0fd00dee78e24579d1abccfe692ca4c9", "text": "7\n\nEND", "start_char_idx": 1791, "end_char_idx": 1797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"5018da1e-a4f1-4731-b55c-151776e5927a": {"node_ids": ["424f04aa-2b6e-4f9b-8141-549b4bd43b9d", "4562a98a-6684-4c1d-9d47-ad75db7b64da", "f0d36b74-c870-4c87-9be4-dd3a94f30107", "5346372c-a1e8-4904-89d3-ff2c9bf63223", "6a391770-56cc-4ef2-b9b5-e9bac03c2ed0", "64abcaea-e874-407d-a515-186b2a75e80b", "4dc6ee0c-973a-4a88-a61d-be060d359dd3", "5cba0685-42ff-4095-9e19-5b303ea80d5a", "67cd0149-92f8-4aed-b957-7467813c2524", "1f11f84d-a003-4bad-82eb-597dde8622b9", "f2d122ce-daf0-4246-b2fe-3ee781826130", "6cbb2bdd-2bb4-4bc0-a125-652a4fa52114", "e92abd3a-60ff-4cc8-a5d5-1705741432dd"], "metadata": {"window": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n For NumPy minibatching is not so much about parallelism as\nabout making the vector operations larger so that the vector\noperations dominate the slowness of Python.  On a GPU mini-\nbatching allows parallelism over the batch elements.\n", "original_text": "TTIC 31230, Fundamentals of Deep Learning\nDavid McAllester, Winter 2020\nMinibatching: The Batch Index\n1\n\nMinibatching\nWe run some number of instances together (or in parallel) and\nthen do a parameter update based on the average gradients of\nthe instances of the batch.\n"}}}}